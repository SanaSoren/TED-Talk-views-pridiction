{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "bqVZBxrvPD8n",
        "x71ZqKXriCWQ",
        "H0kj-8xxnORC"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanaSoren/TED-Talk-views-pridiction/blob/main/TED_Talk_Views_Prediction_Regression_CapstonProject_sanasoren.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  **TED Talk Views Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "ILM16xV1PD8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Regression\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "bqVZBxrvPD8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this project is to develop a predictive model that accurately forecasts the number of views a TED talk video will have. Through the analysis of past TED talks and their associated video metrics, the model will be able to identify trends in viewership and suggest ways to better optimize for higher views. By utilizing the data of past TED talks, the model will be able to create a predictive tool that will allow TED producers to better understand which topics and speakers will be more likely to attract a larger audience."
      ],
      "metadata": {
        "id": "jivUA-ONOwse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import math\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from datetime import datetime\n",
        "import calendar\n",
        "\n",
        "import ast\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.linear_model import Lasso, Ridge\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        " \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "TxPZl-OUKID-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "TED_talk = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Ted Talk Supervised Learning Project/data_ted_talks.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Let's Describe our Data first**"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First \n",
        "TED_talk.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns \n",
        "TED_talk.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "TED_talk.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for Duplicate Value\n",
        "len(TED_talk[TED_talk.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform well on predicting TED Talks video views, I try to use as much features from the dataset as possible. Nevertheless, I have decided not to use some of the parameters (e.g., url, speaker_1 name, etc.) because they won't be much useful in predicting the views. Using some of the features (such as description, topics) I've left for further work.\n",
        "\n",
        "♦ **Talk_ID -** Id of the speaker\n",
        "\n",
        "♦ **Title -** Title of the Talk\n",
        "\n",
        "♦ **Speaker_1 -** Name of the speaker that leads the talk, we rarely see the same speaker do more than 1 talk\n",
        "\n",
        "♦ **All_Speakers -** Name of the talk, which includes the name speaker_1 and title of the talk \n",
        "\n",
        "♦ **About_Speakers -** Name of the talk, which includes the name speaker_1 and the occupation of speaker\n",
        "\n",
        "♦ **Views -** Number of times the video has been watched\n",
        "\n",
        "♦ **Event -** Name of the event of which the talk is part of\n",
        "\n",
        "♦ **Native_Lang -** Number of language in which the talk \n",
        "\n",
        "♦ **Available_Lang -** Number of languages in which the talk is available in\n",
        "\n",
        "♦ **Duration -** Duration of the video\n",
        "\n",
        "♦ **Recorded_date, Published_date -** Date of recording and publishing the talk, from which we get:\n",
        "\n",
        "        ▪ Day of the week\n",
        "        ▪ Month\n",
        "        ▪ Year\n",
        "\n",
        "♦ **Related-talks -** An array that consists of 6 related talks, from which I extract the average number of views.\n",
        "\n",
        "I've excluded the **comments** and **ratings** features, as using those I consider cheating. The point of the task is to predict the number of views for a video which has just been released or is yet to be released. After going through the data analysis notebooks I mentined earlier, I decided to exclude the following features:\n",
        "\n",
        "♦ **Comments -** number of comments on the video\n",
        "\n",
        "♦ **Url -** Url link to the talk\n",
        "\n",
        "The following features I leave for future work:\n",
        "\n",
        "♦ **Description -** Description of the talk, will need to encode this information\n",
        "\n",
        "♦ **Transcript -** Transcript of the talk, will need to encode this information\n",
        "\n",
        "♦ **Topics -** Topics that are associated with the talk\n",
        "\n",
        "♦ **Occupation -** Occupation of the speaker"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TED Talks Data Analysis**\n",
        "\n",
        "### **Cleaning The Data**\n",
        "Various datasets frequently have missing values, so I start off by checking whether the TED Talks dataset has any."
      ],
      "metadata": {
        "id": "hXRbJxk3Nd4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Finding number of unique and null values in each columns\n",
        "pd.DataFrame([[col, TED_talk[col].nunique(), TED_talk[col].isna().sum()]  for  col  in TED_talk],\n",
        "             columns = ['Column Name', 'Unique Count', 'Missing Count'])"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(TED_talk.isnull(), cbar= False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What we get to know about dataset?**\n",
        "\n",
        "There are many null values: Occupations 522 null values, about_speakers 503 null values, Comments 655 null values, all_speakers 4 null values, and recorder_date only 1 null value."
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Now we can start our Exploratory Data Analysis**"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "TED_talk.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fromating DateTime**"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "today=datetime.now()\n",
        "today.strftime('%Y-%m-%d')"
      ],
      "metadata": {
        "id": "4tBngH_hPOA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The format of recorded_date and published_date are in string format, we have to convert them into date format"
      ],
      "metadata": {
        "id": "60D6wWsRP2NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recorded date formatting:\n",
        "TED_talk['recorded_date']= pd.to_datetime(TED_talk['recorded_date'])\n",
        "\n",
        "# Published date formatting:\n",
        "TED_talk['published_date']= pd.to_datetime(TED_talk['published_date'])"
      ],
      "metadata": {
        "id": "PaPWuHPHbnEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TED_talk[['recorded_date','published_date']].info()"
      ],
      "metadata": {
        "id": "NrK899PLgN2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of days Ted talk has been published\n",
        "last_publishing_date= TED_talk['published_date'].max()\n",
        "TED_talk['time_passed_since_published']= last_publishing_date - pd.DatetimeIndex(TED_talk['published_date'])"
      ],
      "metadata": {
        "id": "-d-_JWvIbpZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import calendar\n",
        "import datetime\n",
        "\n",
        "month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "day_order   = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "\n",
        "# Create new columns for publish_month, publish_year, publish_day, publish_week_day\n",
        "TED_talk['publish_month'] = pd.DatetimeIndex(TED_talk['published_date']).month\n",
        "TED_talk['publish_month'] = TED_talk['publish_month'].apply(lambda x: calendar.month_abbr[x])\n",
        "TED_talk['publish_year'] = pd.DatetimeIndex(TED_talk['published_date']).year\n",
        "TED_talk['publish_day'] = pd.DatetimeIndex(TED_talk['published_date']).day\n",
        "TED_talk['publish_week_day'] = TED_talk['published_date'].apply(lambda x: day_order[datetime.date(x.year, x.month, x.day).weekday()])\n"
      ],
      "metadata": {
        "id": "uthq-I1hQjBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#month_order = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "#day_order   = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "\n",
        "#TED_talk['publish_month'] = pd.DatetimeIndex(TED_talk['published_date']).month\n",
        "#TED_talk['publish_month'] = TED_talk['publish_month'].apply(lambda x: calendar.month_abbr[x])\n",
        "#TED_talk['publish_year'] = pd.DatetimeIndex(TED_talk['published_date']).year\n",
        "#TED_talk['publish_day'] = pd.DatetimeIndex(TED_talk['published_date']).day\n",
        "#TED_talk['publish_week_day']= TED_talk['published_date'].apply(lambda x: day_order[datetime.date(x.year, x.month, x.day).weekday()])"
      ],
      "metadata": {
        "id": "4xoF-yzBbpb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating variable for Daily Views(Target)**"
      ],
      "metadata": {
        "id": "HvsI7VcrQ-V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Daily views/Talk:\n",
        "TED_talk['daily_views'] = TED_talk['views'] / ( TED_talk['time_passed_since_published'].apply(lambda x : x.days) + 1 )"
      ],
      "metadata": {
        "id": "t9qYRB4PbpWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TED_talk[['publish_month','publish_year','publish_day','publish_week_day','daily_views']].head()"
      ],
      "metadata": {
        "id": "c7oc-9h4bpUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables**"
      ],
      "metadata": {
        "id": "8kh5T4SvVtLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Univariate Analysis**\n",
        "**Why do you do a univariate analysis?**\n",
        "\n",
        "▪ Univariate analysis is a statistical technique used to identify patterns and relationships within a single variable. \n",
        "\n",
        "▪ Univariate analysis can help to assess the distribution of data, identify outliers, and gain a better understanding of the data set as a whole.\n",
        "\n",
        "▪ It can also provide insight into the potential influence of a single variable on a dependent variable.\n",
        "\n",
        "### **Continuous variables**\n"
      ],
      "metadata": {
        "id": "_ZSaRsZgRekG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.title(\"views\")\n",
        "sns.distplot(x= TED_talk['views'])\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.title(\"number of comments\")\n",
        "sns.distplot(x= TED_talk['comments'])\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title(\"duration of talk\")\n",
        "sns.distplot(x= TED_talk['duration'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kvLVN_t-bpSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bivariate analysis with dependent variable**\n",
        "**What is a dependent variable in data analysis?**\n",
        "\n",
        "▪ A dependent variable is a variable in a data analysis that is affected by the changes in an independent variable.\n",
        "\n",
        "▪ It is the variable that is being measured or tested in an experiment.\n",
        "\n",
        "\n",
        "\n",
        "**speaker vs Duration**"
      ],
      "metadata": {
        "id": "46xnLGkgSKJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = TED_talk.groupby(['speaker_1'],as_index=False)['duration'].sum().sort_values('duration',ascending=False)[:25]\n",
        "temp = TED_talk.groupby(['speaker_1'],as_index=False).agg({'duration':'sum','talk_id':'count'}).sort_values('duration',ascending=False).reset_index()[:8]\n",
        "temp['talk_id']=temp['duration']/temp['talk_id']\n",
        "plt.figure(figsize=(15,6))\n",
        "ax=sns.barplot(x='speaker_1',y='duration',data=temp)\n",
        "labels=ax.get_xticklabels()\n",
        "plt.setp(labels, rotation = 60);\n"
      ],
      "metadata": {
        "id": "m0e_W0upSJGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Speaker Vs Number of talks delivered**"
      ],
      "metadata": {
        "id": "0LeYyeHmTqCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_speaker_count= pd.DataFrame(TED_talk['speaker_1'].value_counts()).reset_index().rename(columns=({'index':'Speaker','speaker_1':'Number of talks'}))\n",
        "\n",
        "most_talks = data_speaker_count.nlargest(5, 'Number of talks')\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x = 'Speaker', y = 'Number of talks', data = most_talks)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W5SXa6gdT5Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **speaker_1 vs daily_views**"
      ],
      "metadata": {
        "id": "D8H-aOdOUbLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Top 25 speakers\n",
        "temp = TED_talk.groupby(['speaker_1'],as_index=False)['daily_views'].sum().sort_values('daily_views',ascending=False)[:5]\n",
        "plt.figure(figsize=(8,6))\n",
        "ax=sns.barplot(x='speaker_1', y='daily_views',data=temp)\n",
        "plt.setp(ax.get_xticklabels(), rotation=50);\n",
        "plt.title('Top 5 speaker according to daily_views')\n",
        "ax.grid(False)"
      ],
      "metadata": {
        "id": "sW6BuYqQUxIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Speaker vs Comments**"
      ],
      "metadata": {
        "id": "EI4kPli0U8TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = TED_talk.groupby(['speaker_1'],as_index=False)['comments'].sum().sort_values('comments',ascending=False)[:5]\n",
        "plt.figure(figsize=(10,6))\n",
        "ax=sns.barplot(x='speaker_1',y='comments',data=temp);\n",
        "plt.setp(ax.get_xticklabels(), rotation=70);\n",
        "plt.title('Most popular speaker according to views',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9C8LpcNvU7rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Speaker vs Average Views**"
      ],
      "metadata": {
        "id": "VQd05e-BVRjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Speaker most popular video\n",
        "temp = TED_talk[['speaker_1','views']].sort_values('views',ascending=False)[:5]\n",
        "plt.figure(figsize=(8,6))\n",
        "ax=sns.barplot(x='speaker_1',y='views',data=temp)\n",
        "plt.setp(ax.get_xticklabels(), rotation=60);\n",
        "plt.title('Speaker_1 with most popular video')\n",
        "plt.ylabel('Average views in ten millions')\n",
        "ax.grid(False)"
      ],
      "metadata": {
        "id": "tZDChSoVVXLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Target Encoding**\n",
        "Target encoding is a technique used in data analysis to encode categorical variables into numerical values. This is useful when dealing with categorical variables (i.e. variables with a finite number of levels) as it allows for easier interpretation of the data, as well as a more accurate analysis of the data. Target encoding can also be used to reduce the amount of overfitting in a model, as it replaces the many levels of a categorical variable with a single numerical value.\n",
        "\n",
        "### **Applying Target encoding on speaker_1**"
      ],
      "metadata": {
        "id": "HbqOqBHlV-Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speaker = TED_talk.groupby('speaker_1').agg({'daily_views' : 'mean'}).sort_values(['daily_views'],ascending=False)\n",
        "speaker = speaker.to_dict()\n",
        "speaker = speaker.values()\n",
        "speaker =  list(speaker)[0]\n",
        "TED_talk['speaker_1 average views'] = TED_talk['speaker_1'].map(speaker)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(TED_talk['speaker_1 average views'])\n",
        "plt.show()\n",
        "     "
      ],
      "metadata": {
        "id": "GBoVgEAnWibi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Event**\n",
        "Event is also a catagorical variable, therefore we also apply target encoding on it."
      ],
      "metadata": {
        "id": "U42Oka2DXe6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "event = TED_talk.groupby('event').agg({'daily_views' : 'mean'}).sort_values(['daily_views'],ascending=False)\n",
        "event = event.to_dict()\n",
        "event = event.values()\n",
        "event=  list(event)[0]\n",
        "TED_talk['Event wise Average Views']= TED_talk['event'].map(event)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(TED_talk['Event wise Average Views'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HR3Y2zqIXeLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Top 10 TED Talk Events**"
      ],
      "metadata": {
        "id": "PyXO-JKGYCom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp = TED_talk.groupby(['event','publish_year'],as_index=False).agg({'daily_views':'sum','talk_id':'count'}).sort_values('daily_views',ascending=False).reset_index()[:8]\n",
        "temp['talk_id'] = temp['daily_views']/temp['talk_id']\n",
        "plt.figure(figsize=(10,6))\n",
        "ax = sns.barplot(x='event',y='daily_views',data=temp)\n",
        "labels = ax.get_xticklabels()\n",
        "plt.title('Top TED Events by daily views')\n",
        "plt.ylabel('daily views in million')\n",
        "plt.setp(labels, rotation=50);\n",
        "     "
      ],
      "metadata": {
        "id": "LxzKDmu_YCMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Top TED Events by Average daily views**"
      ],
      "metadata": {
        "id": "0MiAJktBYB1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,6))\n",
        "ax = sns.barplot(x='event',y='talk_id',data=temp)\n",
        "labels = ax.get_xticklabels()\n",
        "plt.title('Top TED Events by Average daily views')\n",
        "plt.xlabel('Events')\n",
        "plt.ylabel('Average daily views in millions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LvkDSI3zYnUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Available Language Variable**"
      ],
      "metadata": {
        "id": "s5YOfoCzZReT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TED_talk['number of language'] = TED_talk['available_lang'].apply(lambda x: len(x))\n",
        "sns.distplot(TED_talk['number of language'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dIVK2NnWZZTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Number of Topics from Topic variable**"
      ],
      "metadata": {
        "id": "UIWE4yPXZhDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TED_talk['topics'] = TED_talk.apply(lambda x: eval(\"x['topics']\"), axis=1)\n",
        "TED_talk['number of topics'] = TED_talk.apply(lambda x: len(x['topics']), axis=1)\n",
        "# graph:\n",
        "plt.figure(figsize=(15,5))\n",
        "sns.distplot(TED_talk['number of topics'])"
      ],
      "metadata": {
        "id": "2TvTn2mMZ_NM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Number of Unique Topics**"
      ],
      "metadata": {
        "id": "3WIzGi_Nbvbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for unique topic\n",
        "unique_topics=[]\n",
        "for i in range(0,len(TED_talk)):\n",
        "  temp = TED_talk['topics'][i]\n",
        "  for i in temp:\n",
        "    if(i not in unique_topics):\n",
        "      unique_topics.append(i)\n",
        "      \n",
        "len(unique_topics)"
      ],
      "metadata": {
        "id": "syN30WAFbueJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Target encoding on Unique Topics**"
      ],
      "metadata": {
        "id": "ZqRJiDGfb-Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetching the average views with respect to each topic in another dict unique_topics_avg_view_dict\n",
        "unique_topics_avg_view_dict={}\n",
        "for topic in unique_topics:\n",
        "  temp=0\n",
        "  count=0\n",
        "  for i in range(0,len(TED_talk)):\n",
        "    temp2 = TED_talk['topics'][i]\n",
        "    if(topic in temp2):\n",
        "      temp+= TED_talk['daily_views'][i]\n",
        "      count+=1\n",
        "  unique_topics_avg_view_dict[topic]=temp//count"
      ],
      "metadata": {
        "id": "MgAVOszeb9l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the average views w.r.t topic for each talk\n",
        "topics_wise_avg_views=[]\n",
        "for i in range(0,len(TED_talk)):\n",
        "  temp=0\n",
        "  temp_topic = TED_talk['topics'][i]\n",
        "  for ele in temp_topic:\n",
        "    temp+= unique_topics_avg_view_dict[ele]\n",
        "  \n",
        "  topics_wise_avg_views.append(temp//len(temp_topic))\n",
        "\n",
        "se = pd.Series(topics_wise_avg_views)\n",
        "TED_talk['Topics wise average views'] = se.values\n",
        "\n",
        "# Graph:\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.distplot(TED_talk['Topics wise average views'])"
      ],
      "metadata": {
        "id": "NO5Khh08cfh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Related Talk Variable**\n",
        "\n",
        "Related talk column contains a dictionary containing information about related videos with talk_id as key and video name as it's value. taking mean of all realated talk videos views"
      ],
      "metadata": {
        "id": "8d6YsYzKdKnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TED_talk['related_talks'] = TED_talk['related_talks'].apply(lambda x: ast.literal_eval(x))"
      ],
      "metadata": {
        "id": "O6pxxk5vdPzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a new feature called related_views\n",
        "TED_talk['related_views'] = 0\n",
        "\n",
        "# Iterating through the each row and extracting the value of related_talks\n",
        "for index, row in TED_talk.iterrows():\n",
        "    id_list=list(row['related_talks'].keys())\n",
        "    temp=0\n",
        "    for i in range(len(TED_talk)):\n",
        "      if (TED_talk.loc[i,'talk_id']) in id_list:\n",
        "        temp+= TED_talk.loc[i,'daily_views']\n",
        "\n",
        "    TED_talk.loc[index,'related_views']=temp//6\n",
        "\n",
        "# Graph of related_views column\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.distplot(TED_talk['related_views'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A8g9Z-q0dwIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Converting time passed since published into integer**"
      ],
      "metadata": {
        "id": "lCMyYwSjePD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TED_talk['time_passed_since_published'] = TED_talk['time_passed_since_published'].dt.days.astype('int16')"
      ],
      "metadata": {
        "id": "bDtQEd9ffSfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering and Data Preprocessing**\n",
        "▪ Feature engineering and data processing are important steps in the ML workflow. Feature engineering involves creating, selecting, and transforming features to create an informative dataset for model training. Data processing involves cleaning, normalizing, and preparing the data for model training. Feature engineering is essential for creating a dataset that is suitable for training an ML model, while data processing ensures that the data is in an appropriate format for the model. Together, feature engineering and data processing enable ML models to train on data that is accurate and reliable.\n",
        "\n",
        "### **Verifying OLS assumptions**\n",
        "▪ Verifying OLS (ordinary least squares) assumptions in ML (machine learning) is a critical step in any ML model-building process. OLS assumptions help to ensure that the model is accurately representing the underlying data and that the results of the model are reliable. OLS assumptions include linearity, independence, normality, homoscedasticity, and lack of multicollinearity. By validating these assumptions, you can determine if the data is suitable for OLS regression and can identify areas of improvement that may be needed.\n",
        "\n",
        "## **Linearity**\n",
        "▪ Linearity is a key concept in Machine Learning (ML). It is used to understand and determine relationships between the input variables and the output variables. Linearity helps to identify patterns in data and to make predictions from data. It is also used to determine the effects of changes in input variables on the output variables. Linearity can also be used to simplify the ML models since linear models are easier to interpret and understand."
      ],
      "metadata": {
        "id": "FvcwUrsShTfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for Linearity\n",
        "      \n",
        "fig = plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.subplot(3,4,1)\n",
        "plt.title(\"comments\")\n",
        "sns.scatterplot(TED_talk['comments'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,2)                   \n",
        "plt.title(\"duration\")\n",
        "sns.scatterplot(TED_talk['duration'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,3)\n",
        "plt.title(\"time_passed_since_published\")\n",
        "sns.scatterplot(TED_talk['time_passed_since_published'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,4)\n",
        "plt.title(\"publish_year\")\n",
        "sns.scatterplot(TED_talk['publish_year'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,5)\n",
        "plt.title(\"publish_day\")\n",
        "sns.scatterplot(TED_talk['publish_day'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,6)\n",
        "plt.title(\"speaker_1 average views\")\n",
        "sns.scatterplot(TED_talk['speaker_1 average views'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,7)\n",
        "plt.title(\"Event wise Average Views\")\n",
        "sns.scatterplot(TED_talk['Event wise Average Views'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,8)\n",
        "plt.title(\"number of language\")\n",
        "sns.scatterplot(TED_talk['number of language'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,9)\n",
        "plt.title(\"num of topics\")\n",
        "sns.scatterplot(TED_talk['number of topics'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,10)\n",
        "plt.title(\"Topics wise average views\")\n",
        "sns.scatterplot(TED_talk['Topics wise average views'],TED_talk['daily_views'])\n",
        "\n",
        "plt.subplot(3,4,11)\n",
        "plt.title(\"related_views\")\n",
        "sns.scatterplot(TED_talk['related_views'],TED_talk['daily_views'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cwQOgCJDhxpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformation for Linearity**"
      ],
      "metadata": {
        "id": "UJ2IilGRkd8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation\n",
        "TED_talk['log_daily_views'] = np.log(TED_talk['daily_views'])\n",
        "TED_talk['log_comments'] = np.log(TED_talk['comments'])\n",
        "TED_talk['log_speaker_1_avg_views'] = np.log(TED_talk['speaker_1 average views'])\n",
        "TED_talk['log_event_wise_average_views'] = np.log(TED_talk['Event wise Average Views'])\n",
        "TED_talk['log_duration'] = np.log(TED_talk['duration'])\n",
        "TED_talk['log_topics_wise_average_views'] = np.log(TED_talk['Topics wise average views'])\n",
        "TED_talk['log_related_views'] = np.log(TED_talk['related_views'])"
      ],
      "metadata": {
        "id": "zX_aZnLXUi-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linearity\n",
        "fig = plt.figure(figsize=(15,10))\n",
        "\n",
        "plt.subplot(3,4,1)\n",
        "plt.title(\"log_comments\")\n",
        "sns.scatterplot(TED_talk['log_comments'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,2)\n",
        "plt.title(\"log_duration\")\n",
        "sns.scatterplot(TED_talk['log_duration'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,3)\n",
        "plt.title(\"time_passed_since_published\")\n",
        "sns.scatterplot(TED_talk['time_passed_since_published'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,4)\n",
        "plt.title(\"publish_year\")\n",
        "sns.scatterplot(TED_talk['publish_year'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,5)\n",
        "plt.title(\"publish_day\")\n",
        "sns.scatterplot(TED_talk['publish_day'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,6)\n",
        "plt.title(\"log_speaker_1_avg_views\")\n",
        "sns.scatterplot(TED_talk['log_speaker_1_avg_views'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,7)\n",
        "plt.title(\"log_event_wise_average_views\")\n",
        "sns.scatterplot(TED_talk['log_event_wise_average_views'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,8)\n",
        "plt.title(\"number_of_lang\")\n",
        "sns.scatterplot(TED_talk['number of language'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,9)\n",
        "plt.title(\"num_of_topics\")\n",
        "sns.scatterplot(TED_talk['number of topics'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,10)\n",
        "plt.title(\"log_topics_wise_average_views\")\n",
        "sns.scatterplot(TED_talk['log_topics_wise_average_views'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,11)\n",
        "plt.title(\"log_related_views\")\n",
        "sns.scatterplot(TED_talk['log_related_views'],TED_talk['log_daily_views'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u-g8g1YjUl3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ **INFERENCE :** Not all features show linearity with the target and also many feature are showing hetroscedasticity"
      ],
      "metadata": {
        "id": "LFaTBeJtmxvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outliers Detection**"
      ],
      "metadata": {
        "id": "Dty_rcbdnBM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplots\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.subplot(3,4,1)\n",
        "#plt.title(\"log_comments\")\n",
        "sns.boxplot(x= TED_talk['log_comments'])\n",
        "\n",
        "plt.subplot(3,4,2)\n",
        "#plt.title(\"duration\")\n",
        "sns.boxplot(x= TED_talk['log_duration'])\n",
        "\n",
        "plt.subplot(3,4,3)\n",
        "#plt.title(\"time_passed_since_published\")\n",
        "sns.boxplot(x= TED_talk['time_passed_since_published'])\n",
        "\n",
        "plt.subplot(3,4,4)\n",
        "#plt.title(\"publish_year\")\n",
        "sns.boxplot(x= TED_talk['publish_year'])\n",
        "\n",
        "plt.subplot(3,4,5)\n",
        "#plt.title(\"publish_day\")\n",
        "sns.boxplot(x= TED_talk['publish_day'])\n",
        "\n",
        "plt.subplot(3,4,6)\n",
        "#plt.title(\"log_speaker_1_avg_views\")\n",
        "sns.boxplot(x= TED_talk['log_speaker_1_avg_views'])\n",
        "\n",
        "plt.subplot(3,4,7)\n",
        "#plt.title(\"log_event_wise_avg_views\")\n",
        "sns.boxplot(x= TED_talk['log_event_wise_average_views'])\n",
        "\n",
        "\n",
        "plt.subplot(3,4,8)\n",
        "#plt.title(\"number_of_lang\")\n",
        "sns.boxplot(x= TED_talk['number of language'])\n",
        "\n",
        "plt.subplot(3,4,9)\n",
        "#plt.title(\"num_of_topics\")\n",
        "sns.boxplot(x= TED_talk['number of topics'])\n",
        "\n",
        "plt.subplot(3,4,10)\n",
        "#plt.title(\"log_daily_views\")\n",
        "sns.boxplot(x= TED_talk['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,11)\n",
        "#plt.title(\"log_topics_wise_avg_views\")\n",
        "sns.boxplot(x= TED_talk['log_topics_wise_average_views'])\n",
        "\n",
        "plt.subplot(3,4,12)\n",
        "#plt.title(\"log_related_views\")\n",
        "sns.boxplot(x= TED_talk['log_related_views'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yNmertqjnA00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing outliers from log_comments\n",
        "q_low = TED_talk['log_comments'].quantile(0.01)\n",
        "q_hi  = TED_talk['log_comments'].quantile(0.99)\n",
        "\n",
        "df_1 = TED_talk[(TED_talk['log_comments'] < q_hi) & (TED_talk['log_comments'] > q_low)]\n",
        "\n",
        "# removing outliers from log_duration\n",
        "q_low = df_1[\"log_duration\"].quantile(0.01)\n",
        "q_hi  = df_1[\"log_duration\"].quantile(0.99)\n",
        "\n",
        "df_2 = df_1[(df_1[\"log_duration\"] < q_hi) & (df_1[\"log_duration\"] > q_low)]\n",
        "\n",
        "# removing outliers from log_speaker_1_avg_views\n",
        "q_low = df_2[\"log_speaker_1_avg_views\"].quantile(0.01)\n",
        "q_hi  = df_2[\"log_speaker_1_avg_views\"].quantile(0.99)\n",
        "\n",
        "df_3 = df_2[(df_2[\"log_speaker_1_avg_views\"] < q_hi) & (df_2[\"log_speaker_1_avg_views\"] > q_low)]\n",
        "\n",
        "# removing outliers from log_event_wise_avg_views\n",
        "q_low = df_3[\"log_event_wise_average_views\"].quantile(0.01)\n",
        "q_hi  = df_3[\"log_event_wise_average_views\"].quantile(0.99)\n",
        "\n",
        "df_4 = df_3[(df_3[\"log_event_wise_average_views\"] < q_hi) & (df_3[\"log_event_wise_average_views\"] > q_low)]\n",
        "\n",
        "# removing outliers from number_of_lang\n",
        "q_low = df_4[\"number of language\"].quantile(0.01)\n",
        "q_hi  = df_4[\"number of language\"].quantile(0.99)\n",
        "\n",
        "df_5 = df_4[(df_4[\"number of language\"] < q_hi) & (df_4[\"number of language\"] > q_low)]\n",
        "     \n",
        "\n",
        "# removing outliers from num_of_topics\n",
        "q_hi  = df_5[\"number of topics\"].quantile(0.99)\n",
        "\n",
        "df_6 = df_5[df_5[\"number of topics\"] < q_hi]\n",
        "\n",
        "# removing outliers from log_daily_views\n",
        "q_low = df_6[\"log_daily_views\"].quantile(0.01)\n",
        "q_hi  = df_6[\"log_daily_views\"].quantile(0.99)\n",
        "\n",
        "df_7 = df_6[(df_6[\"log_daily_views\"] < q_hi) & (df_6[\"log_daily_views\"] > q_low)]\n",
        "\n",
        "# removing outliers from log_topics_wise_avg_views\n",
        "q_low = df_7[\"log_topics_wise_average_views\"].quantile(0.01)\n",
        "q_hi  = df_7[\"log_topics_wise_average_views\"].quantile(0.99)\n",
        "\n",
        "df_8 = df_7[(df_7[\"log_topics_wise_average_views\"] < q_hi) & (df_7[\"log_topics_wise_average_views\"] > q_low)]\n",
        "\n",
        "# removing outliers from log_related_views\n",
        "q_low = df_8[\"log_related_views\"].quantile(0.01)\n",
        "q_hi  = df_8[\"log_related_views\"].quantile(0.99)\n",
        "\n",
        "df_filtered = df_8[(df_8[\"log_related_views\"] < q_hi) & (df_8[\"log_related_views\"] > q_low)]"
      ],
      "metadata": {
        "id": "09paU17nnmyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# New Boxplots\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.subplot(3,4,1)\n",
        "#plt.title(\"log_comments\")\n",
        "sns.boxplot(x= df_filtered['log_comments'])\n",
        "\n",
        "plt.subplot(3,4,2)\n",
        "#plt.title(\"duration\")\n",
        "sns.boxplot(x= df_filtered['log_duration'])\n",
        "\n",
        "plt.subplot(3,4,3)\n",
        "#plt.title(\"time_passed_since_published\")\n",
        "sns.boxplot(x= df_filtered['time_passed_since_published'])\n",
        "\n",
        "plt.subplot(3,4,4)\n",
        "#plt.title(\"publish_year\")\n",
        "sns.boxplot(x= df_filtered['publish_year'])\n",
        "\n",
        "plt.subplot(3,4,5)\n",
        "#plt.title(\"publish_day\")\n",
        "sns.boxplot(x= df_filtered['publish_day'])\n",
        "\n",
        "plt.subplot(3,4,6)\n",
        "#plt.title(\"log_speaker_1_avg_views\")\n",
        "sns.boxplot(x= df_filtered['log_speaker_1_avg_views'])\n",
        "\n",
        "plt.subplot(3,4,7)\n",
        "#plt.title(\"log_event_wise_avg_views\")\n",
        "sns.boxplot(x= df_filtered['log_event_wise_average_views'])\n",
        "\n",
        "\n",
        "plt.subplot(3,4,8)\n",
        "#plt.title(\"number_of_lang\")\n",
        "sns.boxplot(x= df_filtered['number of language'])\n",
        "\n",
        "plt.subplot(3,4,9)\n",
        "#plt.title(\"num_of_topics\")\n",
        "sns.boxplot(x= df_filtered['number of topics'])\n",
        "\n",
        "plt.subplot(3,4,10)\n",
        "#plt.title(\"log_daily_views\")\n",
        "sns.boxplot(x= df_filtered['log_daily_views'])\n",
        "\n",
        "plt.subplot(3,4,11)\n",
        "#plt.title(\"log_topics_wise_avg_views\")\n",
        "sns.boxplot(x= df_filtered['log_topics_wise_average_views'])\n",
        "\n",
        "plt.subplot(3,4,12)\n",
        "#plt.title(\"log_related_views\")\n",
        "sns.boxplot(x= df_filtered['log_related_views'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "amEQbyHboogZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing Irrelevent Features**\n"
      ],
      "metadata": {
        "id": "XvVrEu1spoZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.columns"
      ],
      "metadata": {
        "id": "61cgJcyfpzyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unwanted_features=['talk_id', 'title', 'speaker_1', 'all_speakers', 'occupations',\n",
        "       'about_speakers', 'views', 'recorded_date', 'published_date', 'event',\n",
        "       'native_lang', 'available_lang', 'topics',\n",
        "       'related_talks', 'url', 'description', 'transcript', 'comments', 'duration', 'daily_views','speaker_1 average views',\n",
        "       'Event wise Average Views','Topics wise average views','related_views']"
      ],
      "metadata": {
        "id": "3u7tmq1rp68D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.drop(columns=unwanted_features,inplace=True)"
      ],
      "metadata": {
        "id": "_XpGTQfgqAi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing Collinearity**"
      ],
      "metadata": {
        "id": "4q_UgrZDqR8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(20,10))\n",
        "sns.heatmap(np.abs(df_filtered.corr()), annot= True, cmap= 'GnBu_r',ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PR2DejhsqYql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Variance Inflation Factor Analysis**"
      ],
      "metadata": {
        "id": "WRRHpEEHq_-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vif_data = df_filtered.drop(['publish_week_day','publish_month','log_daily_views','publish_year','number of language','log_comments','log_related_views','log_topics_wise_average_views','log_duration','log_event_wise_average_views'],axis=1)"
      ],
      "metadata": {
        "id": "yG1fr-Y5rK1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vif_df=pd.DataFrame()\n",
        "vif_df['features']=vif_data.columns\n",
        "vif_df['VIF']=[variance_inflation_factor(vif_data.values,i) for i in range(vif_data.shape[1])]\n",
        "vif_df"
      ],
      "metadata": {
        "id": "R0Q_PQozrWlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lets check for normal distribution of features in data**"
      ],
      "metadata": {
        "id": "G7VDXEuDs8TN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting distributions of features\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.subplot(2,3,1)\n",
        "plt.title(\"time_passed_since_published\")\n",
        "sns.distplot(x= df_filtered['time_passed_since_published'])\n",
        "\n",
        "plt.subplot(2,3,2)\n",
        "plt.title(\"publish_day\")\n",
        "sns.distplot(x= df_filtered['publish_day'])\n",
        "\n",
        "plt.subplot(2,3,3)\n",
        "plt.title(\"num_of_topics\")\n",
        "sns.distplot(x= df_filtered['number of topics'])\n",
        "\n",
        "plt.subplot(2,3,4)\n",
        "plt.title(\"log_speaker_1_avg_views\")\n",
        "sns.distplot(x= df_filtered['log_speaker_1_avg_views'])\n",
        "\n",
        "#plt.subplot(2,3,5)\n",
        "#plt.title(\"log_daily_views\")\n",
        "#sns.histplot(x= np.log(df_filtered['related_views']))\n",
        "\n",
        "plt.subplot(2,3,5)\n",
        "plt.title(\"log_daily_views\")\n",
        "sns.distplot(x= df_filtered['log_daily_views'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jHkwToswtDR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformation**"
      ],
      "metadata": {
        "id": "Xd--0zuhtVv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation\n",
        "df_filtered['sqrt_publish_day']=np.sqrt(df_filtered['publish_day'])\n",
        "df_filtered['log_num_of_topics']=np.log(df_filtered['number of topics'])\n",
        "df_filtered['log_time_passed_since_published']=np.log(df_filtered['time_passed_since_published'])"
      ],
      "metadata": {
        "id": "avr-Hfcwta-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ploting distributions of features\n",
        "\n",
        "fig = plt.figure(figsize=(10,8))\n",
        "\n",
        "plt.subplot(2,3,1)\n",
        "plt.title(\"log_time_passed_since_published\")\n",
        "sns.distplot(x= df_filtered['log_time_passed_since_published'])\n",
        "\n",
        "plt.subplot(2,3,2)\n",
        "plt.title(\"sqrt_publish_day\")\n",
        "sns.distplot(x= df_filtered['sqrt_publish_day'])\n",
        "\n",
        "plt.subplot(2,3,3)\n",
        "plt.title(\"log_num_of_topics\")\n",
        "sns.distplot(x= df_filtered['log_num_of_topics'])\n",
        "\n",
        "plt.subplot(2,3,4)\n",
        "plt.title(\"log_speaker_1_avg_views\")\n",
        "sns.distplot(x= df_filtered['log_speaker_1_avg_views'])\n",
        "\n",
        "plt.subplot(2,3,5)\n",
        "plt.title(\"log_daily_views\")\n",
        "sns.distplot(x= df_filtered['log_daily_views'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyaXbr4UtfBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.columns"
      ],
      "metadata": {
        "id": "qcbSwF49t4u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df_filtered.drop(['log_topics_wise_average_views','time_passed_since_published','publish_year','log_duration','log_comments','log_event_wise_average_views','number of language','publish_day','number of topics','log_related_views'],axis=1)"
      ],
      "metadata": {
        "id": "qKhJilm8uOkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "OwAKSBznuZD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lets Start The Model Implementation**"
      ],
      "metadata": {
        "id": "iJJ27WbfujzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['log_daily_views'].describe()"
      ],
      "metadata": {
        "id": "XOUH9pKQuwuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Removing null values from dataset**"
      ],
      "metadata": {
        "id": "HmGwW_bMu2R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dummy = pd.get_dummies(data,drop_first=True)\n",
        "data_dummy.shape"
      ],
      "metadata": {
        "id": "R9-YmqV0uyl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining dependent and independent features**"
      ],
      "metadata": {
        "id": "tQB_p2SEvEGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = data_dummy['log_daily_views']\n",
        "X = data_dummy.drop(columns='log_daily_views')"
      ],
      "metadata": {
        "id": "JVqistK3vBsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "RxKy14GQvBos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Next we will standardize the features**"
      ],
      "metadata": {
        "id": "h8-xOemivSoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "x = scaler.transform(X)"
      ],
      "metadata": {
        "id": "ZeZQZhJCvBl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lets split the data into training and testing**"
      ],
      "metadata": {
        "id": "d8wEzF1avda5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spliting dataset into training and test\n",
        "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
      ],
      "metadata": {
        "id": "M7uYz0r1vBjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Implementing Linear Regression Training Model**"
      ],
      "metadata": {
        "id": "6HU0q89qvkJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression\n",
        "reg=LinearRegression()\n",
        "reg.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "FjtWrfXkvBgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Accuracy On Train Data**"
      ],
      "metadata": {
        "id": "nQDonqrzvsAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = reg.predict(x_train)"
      ],
      "metadata": {
        "id": "rqxxHEwovBde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(y_train,y_pred)\n",
        "plt.xlabel('Target(y_train)',fontsize=20)\n",
        "plt.ylabel('Predictions(yhat)',fontsize=20)\n",
        "plt.title('Prediction VS Target',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z52AzwW0vBaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scatter plot must be as close to the 45 degree line from origin as possible for best predictions**"
      ],
      "metadata": {
        "id": "wtHLGRGkwQkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Other way to judge the model\n",
        "sns.distplot(y_train-y_pred)\n",
        "plt.title('Residual PDF',fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rOTPc_2bvBXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Evaluation Metrics**"
      ],
      "metadata": {
        "id": "ucuk1m_bwbT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# R-square to explain the variability our model id able to explain\n",
        "R2 = reg.score(x_train,y_train)\n",
        "R2"
      ],
      "metadata": {
        "id": "-UXUlXC-vBUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjusted R-square\n",
        "n=len(x_train)\n",
        "p=x_train.shape[1]\n",
        "adj_r_sqr=1-((1-reg.score(x_train,y_train))*(n-1)/(n-p-1))\n",
        "adj_r_sqr"
      ],
      "metadata": {
        "id": "ZdjVTkTJvBRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "variability_df = pd.DataFrame({\"R-Square\":R2,\"Adjusted R-Square\":adj_r_sqr},index=[\"Values\"])\n",
        "variability_df"
      ],
      "metadata": {
        "id": "m3ptReCkvBOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lets See The Model Parameters**\n",
        "\n",
        "### **Intercept**"
      ],
      "metadata": {
        "id": "5WZq0u8swvD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reg.intercept_"
      ],
      "metadata": {
        "id": "L1nOCwHHwrl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Rest Of The Parameters**"
      ],
      "metadata": {
        "id": "RH8HzlVGw7NT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary = pd.DataFrame({'Features':X.columns,'Weight':reg.coef_})\n",
        "summary"
      ],
      "metadata": {
        "id": "4Kiqe_pYwrg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Wieghts Interpretation**\n",
        "\n",
        "### **Continuous Variable**\n",
        "1. A **Positive Weight** shows that as the feature increases in value so does the daily_views and log_daily_views variables.\n",
        "2. A **Negative Weight** shows that as the feature increases in value the daily_views and log_daily_views variables decreases in values.\n",
        "\n",
        "### **Dummy Variables**\n",
        "1. A **Positive Weight** shows that the respective catagory is more expensive than the benchmark\n",
        "2. A **Positive Weight** shows that the respective catagory is less expensive than the benchmark"
      ],
      "metadata": {
        "id": "Atn84l5xxIxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING**"
      ],
      "metadata": {
        "id": "1Az1rLIWxvnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_test=reg.predict(x_test)\n",
        "plt.scatter(y_test,y_pred_test,alpha=0.2)\n",
        "plt.xlabel('Expected',fontsize=20)\n",
        "plt.ylabel('Predicted',fontsize=20)\n",
        "plt.title('Daily Views (Prediction / Expected)',fontsize=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GfS3NdAUwrd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(np.exp(y_pred_test))\n",
        "plt.plot(np.array(np.exp(y_test)))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uQTC6brnwrba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pf_df = pd.DataFrame({'Predictions':np.exp(y_pred_test)})\n",
        "pf_df.head()"
      ],
      "metadata": {
        "id": "LM0KFbROwrYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = y_test.reset_index(drop=True)\n",
        "pf_df['Target(expected values)'] = np.exp(y_test)"
      ],
      "metadata": {
        "id": "wILH5-i-wrVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pf_df.head()"
      ],
      "metadata": {
        "id": "zrWDYrD_wrSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pf_df['Residual'] = pf_df['Target(expected values)']-pf_df['Predictions']\n",
        "pf_df['Difference_percentage'] = np.absolute(pf_df['Residual']/pf_df['Target(expected values)']*100)\n",
        "pf_df.describe()"
      ],
      "metadata": {
        "id": "8DnX8-INwrPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error percentage is very less between 25 quartile to 75 quartile that shows our model is working very good on test data.**\n",
        "\n",
        "### **Error metrices**"
      ],
      "metadata": {
        "id": "qIRuzp1RyYhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(np.exp(y_test), np.exp(y_pred_test))"
      ],
      "metadata": {
        "id": "hhT-P2zywrMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RMSE = math.sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred_test)))"
      ],
      "metadata": {
        "id": "rtOWWSxBwrJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Error\n",
        "sum = 0\n",
        "n=len(y_test)\n",
        "# for loop for iteration\n",
        "for ele in range(n):\n",
        "    sum += abs(np.exp(y_test[ele]) - np.exp(y_pred_test[ele]))\n",
        "  \n",
        "MAE = sum/n\n",
        "  \n",
        "# display\n",
        "print(\"Mean absolute error : \" + str(MAE))"
      ],
      "metadata": {
        "id": "5KmmWRz3wrHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAPE = mean_absolute_percentage_error(np.exp(y_test),np.exp(y_pred_test))"
      ],
      "metadata": {
        "id": "NGoWtNbewrEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = r2_score(np.exp(y_test), np.exp(y_pred_test))\n",
        "ar2=1-(1-r2_score(np.exp(y_test), np.exp(y_pred_test)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))\n",
        "     "
      ],
      "metadata": {
        "id": "LHzONmxVwrBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_metric = pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE,MAPE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE','MAPE'])\n",
        "error_metric"
      ],
      "metadata": {
        "id": "XS3ZKUmdwq-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▶ **INFERENCE :** Error metrices show the same observation of low error in the test dataset\n",
        "\n",
        "# **Lets check for overfitting in our model**\n",
        "\n",
        "## **Lasso Regression Model**\n",
        "▪ Lasso regression (Least Absolute Shrinkage and Selection Operator) is a type of regularized linear regression that uses shrinkage, where data values are shrunk towards a central point, like the mean. It is used to reduce model complexity and prevent overfitting by penalizing large coefficients associated with features and by performing feature selection. This can be used to identify the most important predictors in a dataset and is particularly useful when there are a large number of features.\n",
        "\n",
        "### **Running Grid Search Cross Validation**\n",
        "▪ The use of grid search cross-validation in the Lasso regression model helps to find the best combination of parameters for the model. Grid search cross-validation is a method of hyperparameter tuning that involves training and evaluating a model on each combination of hyperparameters in a grid. This helps to optimize the model for the given data set. By using grid search cross-validation, one can identify the optimal combination of hyperparameters for the best results. This is especially important for the Lasso regression model, since its regularization parameter, lambda, can greatly influence the model's performance. Grid search cross-validation helps to identify the best lambda value to use for the model."
      ],
      "metadata": {
        "id": "59hCsOoGzJ5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross validation\n",
        "lasso = Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "lasso_regressor.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "B1u8S_5m0G0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ],
      "metadata": {
        "id": "rYNkcAYe0GuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lasso = lasso_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "KWoUntiS0Goz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.exp(y_pred_lasso))\n",
        "plt.plot(np.exp(np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Expected\"])\n",
        "plt.title('Daily Views (Prediction / Expected)',fontsize=20)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "yhat_test=reg.predict(x_test)\n",
        "plt.scatter(y_test,y_pred_lasso,alpha=0.2)\n",
        "plt.xlabel('Expected',fontsize=20)\n",
        "plt.ylabel('Predicted',fontsize=20)\n",
        "plt.title('Daily Views (Prediction / Expected)',fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_bgEi68R0GjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(np.exp(y_test), np.exp(y_pred_lasso))"
      ],
      "metadata": {
        "id": "LiEvqNAR0GgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RMSE = math.sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred_lasso)))"
      ],
      "metadata": {
        "id": "TChVzmqT0j2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Error\n",
        "sum = 0\n",
        "n=len(y_test)\n",
        "# for loop for iteration\n",
        "for ele in range(n):\n",
        "    sum += abs(np.exp(y_test[ele]) - np.exp(y_pred_lasso[ele]))\n",
        "  \n",
        "MAE = sum/n\n",
        "  \n",
        "# display\n",
        "print(\"Mean absolute error : \" + str(MAE))"
      ],
      "metadata": {
        "id": "iCfyRcuL0jzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAPE = mean_absolute_percentage_error(np.exp(y_test),np.exp(y_pred_lasso))"
      ],
      "metadata": {
        "id": "RRfh2UCe0jwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = r2_score(np.exp(y_test), np.exp(y_pred_lasso))\n",
        "ar2 = 1-(1-r2_score(np.exp(y_test), np.exp(y_pred_lasso)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))"
      ],
      "metadata": {
        "id": "2qe6n-cR0jpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_metric_lasso = pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE,MAPE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE','MAPE'])\n",
        "error_metric_lasso"
      ],
      "metadata": {
        "id": "1BLT_nM10jjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ridge Regression Model**\n",
        "▪ Ridge regression is a type of regularized regression technique that is used to address the problem of multicollinearity in linear regression. It is an extension of least squares regression that uses a penalty term to shrink the magnitude of the coefficients toward zero. This helps to reduce overfitting and improve the generalization of the model. Ridge regression can also be used to identify important features in a dataset.\n",
        "\n",
        "### **Running Grid Search Cross Validation**\n",
        "▪ Grid search cross-validation is used in the Ridge regression model to find the optimal set of hyperparameters that best generalize the model and minimize the prediction error. Grid search cross-validation helps to identify the best combination of hyperparameters by iterating through different combinations of hyperparameters and evaluating the model performance using cross-validation. This approach helps to avoid overfitting and helps to identify the best set of hyperparameters that would lead to the most accurate predictions on unseen data."
      ],
      "metadata": {
        "id": "lmGsR1qV0-df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperprarameter tuning\n",
        "ridge = Ridge()\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "ridge_regressor.fit(x_train,y_train)"
      ],
      "metadata": {
        "id": "FSxOzzum0jfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)"
      ],
      "metadata": {
        "id": "S6T-CgxE0jc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Prediction\n",
        "y_pred_ridge = ridge_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "MF-IfTbb0jZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.exp(y_pred_ridge))\n",
        "plt.plot(np.exp(np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Expected\"])\n",
        "plt.title('Daily Views (Prediction / Expected)',fontsize=20)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "yhat_test = reg.predict(x_test)\n",
        "plt.scatter(y_test,y_pred_ridge,alpha=0.2)\n",
        "plt.xlabel('Expected',fontsize=20)\n",
        "plt.ylabel('Predicted',fontsize=20)\n",
        "plt.title('Daily Views (Prediction / Expected)',fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A1NaVwVF0jWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(np.exp(y_test), np.exp(y_pred_ridge))"
      ],
      "metadata": {
        "id": "KTAoUX9m0jTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RMSE = math.sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred_ridge)))"
      ],
      "metadata": {
        "id": "Nd1-huOo0GdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Error\n",
        "sum = 0\n",
        "n=len(y_test)\n",
        "# for loop for iteration\n",
        "for ele in range(n):\n",
        "    sum += abs(np.exp(y_test[ele]) - np.exp(y_pred_ridge[ele]))\n",
        "  \n",
        "MAE = sum/n\n",
        "  \n",
        "# display\n",
        "print(\"Mean absolute error : \" + str(MAE))"
      ],
      "metadata": {
        "id": "dB1h0IYe1gW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAPE = mean_absolute_percentage_error(np.exp(y_test),np.exp(y_pred_ridge))"
      ],
      "metadata": {
        "id": "_FfE-y1Z1f-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = r2_score(np.exp(y_test), np.exp(y_pred_ridge))\n",
        "ar2=1-(1-r2_score(np.exp(y_test), np.exp(y_pred_ridge)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))"
      ],
      "metadata": {
        "id": "jEwbIYHq1f7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_metric_ridge=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE,MAPE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE','MAPE'])\n",
        "error_metric_ridge"
      ],
      "metadata": {
        "id": "oa_nYDIk1f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Elastic Regression Model**\n",
        "▪ Elastic regression is a machine learning technique used to reduce the amount of high-dimensional data that needs to be processed in order to make a prediction. It is a regularization technique that combines the principles of ridge regression and lasso regression to automatically select important features from a large data set. Elastic regression can help reduce the complexity of a model and improve the accuracy of predictions.\n",
        "\n",
        "### **Running Grid Search Cross Validation**\n",
        "▪ Running Grid Search Cross-Validation in the Elastic regression model is a great way to find the optimal hyperparameters of the model. Grid Search Cross-Validation allows us to test a variety of combinations of hyperparameters, which can help us find the best combination of hyperparameters for the model. This can help us improve the performance of the model and ensure that it is optimized for the task at hand. Additionally, it can help us reduce the chances of overfitting, which can be an issue when using this type of regression model."
      ],
      "metadata": {
        "id": "s-yJj6Sb1yLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "elastic = ElasticNet()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100],'l1_ratio':[0.3,0.4,0.5,0.6,0.7,0.8]}\n",
        "elastic_regressor = GridSearchCV(elastic, parameters, scoring='neg_mean_squared_error',cv=5)\n",
        "elastic_regressor.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "KMn-0Ioh1f1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,elastic_regressor.best_params_)\n",
        "print(\"\\nUsing \",elastic_regressor.best_params_, \" the negative mean squared error is: \", elastic_regressor.best_score_)"
      ],
      "metadata": {
        "id": "MVohxDa21fx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_elastic = elastic_regressor.predict(x_test)"
      ],
      "metadata": {
        "id": "4hWZzbnI1fvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.exp(y_pred_elastic))\n",
        "plt.plot(np.exp(np.array(y_test)))\n",
        "plt.legend([\"Predicted\",\"Expected\"])\n",
        "plt.title('Daily Views (Prediction / Expected)',fontsize=20)\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "yhat_test=reg.predict(x_test)\n",
        "plt.scatter(y_test,y_pred_elastic,alpha=0.2)\n",
        "plt.xlabel('Expected',fontsize=20)\n",
        "plt.ylabel('Predicted',fontsize=20)\n",
        "plt.title('Daily Views (Prediction / Expected)',fontsize=20)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4nMERXMW1frt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(np.exp(y_test), np.exp(y_pred_elastic))"
      ],
      "metadata": {
        "id": "aM9qdeG52GIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RMSE = math.sqrt(mean_squared_error(np.exp(y_test), np.exp(y_pred_elastic)))"
      ],
      "metadata": {
        "id": "hGJg5Flb2GFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Error\n",
        "sum = 0\n",
        "n=len(y_test)\n",
        "# for loop for iteration\n",
        "for ele in range(n):\n",
        "    sum += abs(np.exp(y_test[ele]) - np.exp(y_pred_elastic[ele]))\n",
        "  \n",
        "MAE = sum/n\n",
        "  \n",
        "# display\n",
        "print(\"Mean absolute error : \" + str(MAE))"
      ],
      "metadata": {
        "id": "dpL_wkD82GCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAPE = mean_absolute_percentage_error(np.exp(y_test),np.exp(y_pred_elastic))"
      ],
      "metadata": {
        "id": "CKn0M_-S2F_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = r2_score(np.exp(y_test), np.exp(y_pred_elastic))\n",
        "ar2=1-(1-r2_score(np.exp(y_test), np.exp(y_pred_elastic)))*((x_test.shape[0]-1)/(x_test.shape[0]-x_test.shape[1]-1))"
      ],
      "metadata": {
        "id": "iH81eanI2F8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_metric_Elastic=pd.DataFrame({'Values':[r2,ar2,MSE,RMSE,MAE,MAPE]},index=['R-Square','Adj. R-Square','MSE','RMSE','MAE','MAPE'])\n",
        "error_metric_ridge"
      ],
      "metadata": {
        "id": "RlO-smot2F5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Summary= pd.DataFrame({'Linear Regression':error_metric['Values'],\n",
        "                             'Lasso':error_metric_lasso['Values'],\n",
        "                             'Ridge':error_metric_ridge['Values'],\n",
        "                             'Elastic': error_metric_Elastic['Values']})"
      ],
      "metadata": {
        "id": "AIWDlTE8mwzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Summary"
      ],
      "metadata": {
        "id": "v-lSe7JV2Fzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Conclusion**\n",
        "\n",
        "On comparing all the models our base linear regression model is still performing better followed by Lasso, Ridge, and ElasticNet Regression model on the basis of RMSE. But our model contains a large number of outliers and the value of RMSE is affected by outliers, therefore, we will use MAE as our evaluation matrix according to which Lasso Regressor has the best performance.\n",
        "\n",
        "We can also see that Lasso and Ridge regression models are performing better than the Base Linear Regression Model because of the feature selection methods that are implemented in both models.\n",
        "\n",
        "In conclusion, the Lasso Regression model has the best performance on the given dataset based on the evaluation matrix MAE.\n",
        "\n",
        "###**Future Work**\n",
        "▪ Improve feature engineering\n",
        "\n",
        "▪ Remove unimportant and correlated features\n",
        "\n",
        "▪ Normalise the data\n",
        "\n",
        "▪ Improve the hyperparameters of the models\n",
        "\n",
        "▪ Use PCA"
      ],
      "metadata": {
        "id": "hiYABNf222sV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCaDXywqnBq_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}